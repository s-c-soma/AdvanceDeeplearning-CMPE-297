{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ExtraCredit_Assignment_6_TFX_End2End_Pipeline_IRIS.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-c-soma/AdvanceDeeplearning-CMPE-297/blob/master/ExtraCredit/ExtraCredit_Assignment_6_TFX_End2End_Pipeline_IRIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23R0Z9RojXYW"
      },
      "source": [
        "# TFX Keras Interactive Context: End2End Pipeline Using IRIS Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GivNBNYjb3b"
      },
      "source": [
        "## Implementation Details and Discussion\n",
        "\n",
        "* This colab implements the end to end TFX interactive cotext pipeline for IRIS Dataset\n",
        "\n",
        "* 8 Steps of TFX pipeline has been implemented here. They are:\n",
        "> 1. ExampleGen\n",
        "> 2. StatisticsGen\n",
        "> 3. SchemaGen\n",
        "> 4. ExampleValidator\n",
        "> 5. Transform\n",
        "> 6. Trainer\n",
        "> 7. Evaluator\n",
        "> 8. Pusher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmgi8ZvQkScg"
      },
      "source": [
        "### Upgrade Pip\n",
        "\n",
        "To avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab.  Local systems can of course be upgraded separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as4OTe2ukSqm"
      },
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZOYTt1RW4TK"
      },
      "source": [
        "### Install TFX\n",
        "\n",
        "**Note: In Google Colab, because of package updates, the first time you run this cell you must restart the runtime (Runtime > Restart runtime ...).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4SQA7Q5nej3"
      },
      "source": [
        "!pip install -q -U --use-feature=2020-resolver tfx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIqpWK9efviJ"
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import urllib\n",
        "\n",
        "import absl\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "tf.get_logger().propagate = False\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "import tfx\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import Evaluator\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import Pusher\n",
        "from tfx.components import ResolverNode\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Trainer\n",
        "from tfx.components import Transform\n",
        "from tfx.components.base import executor_spec\n",
        "from tfx.components.trainer.executor import GenericExecutor\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.types import Channel\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.types.standard_artifacts import ModelBlessing\n",
        "from tfx.utils.dsl_utils import external_input\n",
        "\n",
        "\n",
        "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCZTHRy0N1D6"
      },
      "source": [
        "Let's check the library versions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ4K18_DN2D8"
      },
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufJKQ6OvkJlY"
      },
      "source": [
        "### Set up pipeline paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss5PB9Rbdsja"
      },
      "source": [
        "# This is the root directory for your TFX pip package installation.\n",
        "_tfx_root = tfx.__path__[0]\n",
        "\n",
        "# This is the directory containing the TFX Chicago Taxi Pipeline example.\n",
        "_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n",
        "\n",
        "# This is the path where your model will be pushed for serving.\n",
        "_serving_model_dir = os.path.join(\n",
        "    tempfile.mkdtemp(), 'serving_model/taxi_simple')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WV7kySh14fA"
      },
      "source": [
        "_data_root = tempfile.mkdtemp(prefix='tfx-iris')\n",
        "DATA_PATH = 'https://raw.githubusercontent.com/valeriano-manassero/tfx-kubeflow-pipelines/master/iris/data/iris.csv'\n",
        "_data_filepath = os.path.join(_data_root, \"iris.csv\")\n",
        "urllib.request.urlretrieve(DATA_PATH, _data_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blZC1sIQOWfH"
      },
      "source": [
        "Take a quick look at the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5YPeLPFOXaD"
      },
      "source": [
        "!head {_data_filepath}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ONIE_hdkPS4"
      },
      "source": [
        "### Create the InteractiveContext\n",
        "Last, we create an InteractiveContext, which will allow us to run TFX components interactively in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rh6K5sUf9dd"
      },
      "source": [
        "# Here, we create an InteractiveContext using default parameters. This will\n",
        "# use a temporary directory with an ephemeral ML Metadata database instance.\n",
        "# To use your own pipeline root or database, the optional properties\n",
        "# `pipeline_root` and `metadata_connection_config` may be passed to\n",
        "# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n",
        "# notebook.\n",
        "context = InteractiveContext()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPAhmKyOUMi5"
      },
      "source": [
        "# ExampleGen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyXjuMt8f-9u"
      },
      "source": [
        "example_gen = CsvExampleGen(input=external_input(_data_root))\n",
        "context.run(example_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqCoZh7KPUm9"
      },
      "source": [
        "Let's examine the output artifacts of `ExampleGen`. This component produces two artifacts, training examples and evaluation examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9fo16-OTRxU"
      },
      "source": [
        "#StatisticsGen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAscCCYWgA-9"
      },
      "source": [
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "context.run(statistics_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLI6cb_5WugZ"
      },
      "source": [
        "After `StatisticsGen` finishes running, we can visualize the outputted statistics. Try playing with the different plots!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLKLTO9Nk60p"
      },
      "source": [
        "# SchemaGen\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygQvZ6hsiQ_J"
      },
      "source": [
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(schema_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6TxTUKXM6b"
      },
      "source": [
        "After `SchemaGen` finishes running, we can visualize the generated schema as a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec9vqDXpXeMb"
      },
      "source": [
        "context.show(schema_gen.outputs['schema'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1qcUuO9k9f8"
      },
      "source": [
        "# ExampleValidator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRlRUuGgiXks"
      },
      "source": [
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "context.run(example_validator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855mrHgJcoer"
      },
      "source": [
        "After `ExampleValidator` finishes running, we can visualize the anomalies as a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDyAAozQcrk3"
      },
      "source": [
        "context.show(example_validator.outputs['anomalies'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPViEz5RlA36"
      },
      "source": [
        "# Transform\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mjypeKOGw0g"
      },
      "source": [
        "!head {_data_filepath}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AJ9hBs94YJm"
      },
      "source": [
        "_iris_transform_module_file = 'iris_utils.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYmxxx9A4YJn"
      },
      "source": [
        "%%writefile {_iris_transform_module_file}\n",
        "\n",
        "from typing import Text\n",
        "\n",
        "import absl\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "from tfx.components.trainer.executor import TrainerFnArgs\n",
        "\n",
        "\n",
        "_FEATURE_KEYS = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "_LABEL_KEY = 'species'\n",
        "\n",
        "\n",
        "def _transformed_name(key):\n",
        "    return key + '_xf'\n",
        "\n",
        "\n",
        "def _gzip_reader_fn(filenames):\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
        "\n",
        "\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        feature_spec.pop(_LABEL_KEY)\n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        transformed_features.pop(_transformed_name(_LABEL_KEY))\n",
        "        return model(transformed_features)\n",
        "\n",
        "    return serve_tf_examples_fn\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: Text, tf_transform_output: tft.TFTransformOutput, batch_size: int = 200) -> tf.data.Dataset:\n",
        "    transformed_feature_spec = (tf_transform_output.transformed_feature_spec().copy())\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=transformed_feature_spec,\n",
        "        reader=_gzip_reader_fn,\n",
        "        label_key=_transformed_name(_LABEL_KEY))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def _build_keras_model() -> tf.keras.Model:\n",
        "    inputs = [keras.layers.Input(shape=(1,), name=_transformed_name(f)) for f in _FEATURE_KEYS]\n",
        "    d = keras.layers.concatenate(inputs)\n",
        "    for _ in range(3):\n",
        "        d = keras.layers.Dense(8, activation='relu')(d)\n",
        "    output = keras.layers.Dense(3, activation='softmax')(d)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=output)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "        loss='binary_categorical_crossentropy',\n",
        "        metrics=[keras.metrics.Accuracy()])\n",
        "\n",
        "    model.summary(print_fn=absl.logging.info)\n",
        "    return model\n",
        "\n",
        "\n",
        "def _build_keras_model2() -> tf.keras.Model:\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(8, input_dim=4, activation='relu'))\n",
        "    model.add(keras.layers.Dense(10, activation='relu'))\n",
        "    model.add(keras.layers.Dense(8, activation='relu'))\n",
        "    model.add(keras.layers.Dense(3, activation='softmax'))\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=[keras.metrics.CategoricalAccuracy()])\n",
        "\n",
        "    model.summary(print_fn=absl.logging.info)\n",
        "    return model\n",
        "\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "    outputs = {}\n",
        "\n",
        "    for key in _FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = tft.scale_to_z_score(inputs[key])\n",
        "    outputs[_transformed_name(_LABEL_KEY)] = inputs[_LABEL_KEY]\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def run_fn(fn_args: TrainerFnArgs):\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
        "\n",
        "    train_dataset = _input_fn(fn_args.train_files, tf_transform_output, 40)\n",
        "    eval_dataset = _input_fn(fn_args.eval_files, tf_transform_output, 40)\n",
        "\n",
        "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
        "    with mirrored_strategy.scope():\n",
        "        model = _build_keras_model()\n",
        "\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=fn_args.train_steps,\n",
        "        validation_data=eval_dataset,\n",
        "        validation_steps=fn_args.eval_steps)\n",
        "\n",
        "    signatures = {\n",
        "        'serving_default':\n",
        "            _get_serve_tf_examples_fn(model,\n",
        "                                      tf_transform_output).get_concrete_function(\n",
        "                tf.TensorSpec(\n",
        "                    shape=[None],\n",
        "                    dtype=tf.string,\n",
        "                    name='examples')),\n",
        "    }\n",
        "\n",
        "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgbmZr3sgbWW"
      },
      "source": [
        "Now, we pass in this feature engineering code to the `Transform` component and run it to transform your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHfhth_GiZI9"
      },
      "source": [
        "transform = Transform(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      schema=schema_gen.outputs['schema'],\n",
        "      module_file=_iris_transform_module_file)\n",
        "context.run(transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tRw4DneR3i7"
      },
      "source": [
        "train_uri = transform.outputs['transform_graph'].get()[0].uri\n",
        "os.listdir(train_uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBJFtnl6lCg9"
      },
      "source": [
        "# Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzPtyuIsM8FH"
      },
      "source": [
        "trainer = Trainer(\n",
        "      module_file=_iris_transform_module_file,\n",
        "      custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
        "      examples=transform.outputs['transformed_examples'],\n",
        "      transform_graph=transform.outputs['transform_graph'],\n",
        "      schema=schema_gen.outputs['schema'],\n",
        "      train_args=trainer_pb2.TrainArgs(num_steps=500),\n",
        "      eval_args=trainer_pb2.EvalArgs(num_steps=200))\n",
        "\n",
        "context.run(trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmPftrv0lEQy"
      },
      "source": [
        "# Evaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNMTzKH8Vh1r"
      },
      "source": [
        "model_resolver = ResolverNode(instance_name='latest_blessed_model_resolver',\n",
        "                                  resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
        "                                  model=Channel(type=Model),\n",
        "                                  model_blessing=Channel(type=ModelBlessing))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdAt-61DUqYL"
      },
      "source": [
        "eval_config = tfma.EvalConfig(\n",
        "      model_specs=[tfma.ModelSpec(label_key='label')],\n",
        "      slicing_specs=[tfma.SlicingSpec()],\n",
        "      metrics_specs=[\n",
        "          tfma.MetricsSpec(metrics=[\n",
        "              tfma.MetricConfig(\n",
        "                  class_name='BinaryAccuracy',\n",
        "                  threshold=tfma.MetricThreshold(\n",
        "                      value_threshold=tfma.GenericValueThreshold(\n",
        "                          # Increase this threshold when training on complete\n",
        "                          # dataset.\n",
        "                          lower_bound={'value': 0.4}),\n",
        "                      change_threshold=tfma.GenericChangeThreshold(\n",
        "                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                          absolute={'value': -1e-2})))\n",
        "          ])\n",
        "      ])\n",
        "\n",
        "evaluator = Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        "    baseline_model=model_resolver.outputs['model'],\n",
        "    # Change threshold will be ignored if there is no baseline (first run).\n",
        "    eval_config=eval_config)\n",
        "\n",
        "context.run(evaluator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njHA0KYiT_mY"
      },
      "source": [
        "# Pusher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a1V2FJEUyCq"
      },
      "source": [
        "pusher = Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      model_blessing=evaluator.outputs['blessing'],\n",
        "      push_destination=pusher_pb2.PushDestination(\n",
        "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "              base_directory=_serving_model_dir)))\n",
        "\n",
        "context.run(pusher)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}